---
title: "Xue_final"
author: "Xue Yang"
date: "5/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
library(tidyverse)
library(haven)
library(caret)
library(corrplot)
library(caret)
library(e1071)
library(kernlab)
library(factoextra)
```


### Data

```{r}
demographics_2015 = read_xpt("data/demographics.XPT") %>% 
  janitor::clean_names() %>%
  select(seqn, age = ridageyr, gender = riagendr, race = ridreth3, citizen = dmdcitzn, education = dmdeduc2) %>% 
  filter(age > 20)

demographics_2014 = read_xpt("data/demographics_2014.XPT") %>% 
  janitor::clean_names() %>%
  select(seqn, age = ridageyr, gender = riagendr, race = ridreth3, citizen = dmdcitzn, education = dmdeduc2)  %>% 
  filter(age > 20)

demographics = bind_rows(demographics_2014, demographics_2015) %>% 
  mutate(citizen = ifelse(citizen %in% c(1,2), citizen, NA))

income_2015 = read_xpt("data/income.XPT") %>% 
  janitor::clean_names() %>% 
  select(seqn, income = indfmmpc) %>% 
  mutate(income = ifelse(income %in% c(1,2,3), income, NA))

income_2014 = read_xpt("data/income_2014.XPT") %>% 
  janitor::clean_names() %>% 
  select(seqn, income = indfmmpc) %>% 
  mutate(income = ifelse(income %in% c(1,2,3), income, NA))

income = bind_rows(income_2014, income_2015)

depression_2015 = read_xpt("data/mental_health.XPT") %>% 
  janitor::clean_names()

seqn = depression_2015$seqn
depression_subset = depression_2015 %>% select(-seqn)
depression_2015 = map_dfc(depression_subset, ~ifelse(.x %in% c(0,1,2,3), .x, NA)) %>% 
  mutate(depression_score = round(rowMeans(., na.rm = TRUE), 3)) %>% 
  add_column(seqn) %>% 
  select(seqn, depression_score)

depression_2014 = read_xpt("data/mental_health_2014.XPT") %>% 
  janitor::clean_names()
seqn = depression_2014$seqn
depression_subset = depression_2014 %>% select(-seqn)
depression_2014 = map_dfc(depression_subset, ~ifelse(.x %in% c(0,1,2,3), .x, NA)) %>% 
  mutate(depression_score = round(rowMeans(., na.rm = TRUE), 3)) %>% 
  add_column(seqn) %>% 
  select(seqn, depression_score)

depression = bind_rows(depression_2014, depression_2015)

bp_chol_2015 = read_xpt("data/bp_chol.xpt") %>% janitor::clean_names() %>%
  select(seqn, highbp = bpq020, highchol = bpq080) %>%
  mutate(highbp = ifelse(highbp == 9, NA, highbp),
         highchol = ifelse(! (highchol %in% c(1,2)), NA, highchol))

bp_chol_2014 = read_xpt("data/bp_chol_2014.xpt") %>% janitor::clean_names() %>%
  select(seqn, highbp = bpq020, highchol = bpq080) %>%
  mutate(highbp = ifelse(highbp == 9, NA, highbp),
         highchol = ifelse(! (highchol %in% c(1,2)), NA, highchol))

bp_chol = bind_rows(bp_chol_2014, bp_chol_2015)


smoking_2015 = read_xpt("data/smoking.xpt") %>% janitor::clean_names() %>%
  select(seqn, smoked = smq020) %>%
  mutate(smoked = ifelse(!(smoked %in% c(1,2)), NA, smoked))
  
smoking_2014 = read_xpt("data/smoking_2014.xpt") %>% janitor::clean_names() %>%
  select(seqn, smoked = smq020) %>%
  mutate(smoked = ifelse(!(smoked %in% c(1,2)), NA, smoked))
 
smoking = bind_rows(smoking_2014, smoking_2015)                                   


alcohol_2015 = read_xpt("data/alcohol.xpt") %>% janitor::clean_names() %>%
  select(seqn,drinker = alq101, heavy_drinker = alq151 ) %>%
  mutate(heavy_drinker = ifelse(!(heavy_drinker %in% c(1,2)), NA, heavy_drinker),
         drinker = ifelse(!(drinker %in% c(1,2)), NA, drinker))

alcohol_2014 = read_xpt("data/alcohol_2014.xpt") %>% janitor::clean_names() %>%
  select(seqn,drinker = alq101, heavy_drinker = alq151 ) %>%
  mutate(heavy_drinker = ifelse(!(heavy_drinker %in% c(1,2)), NA, heavy_drinker),
         drinker = ifelse(!(drinker %in% c(1,2)), NA, drinker))
alcohol = bind_rows(alcohol_2014, alcohol_2015)


diet_behavior_2015 =
  read_xpt("data/diet.xpt") %>% 
  select(seqn = SEQN, 
         fast = DBD900, 
         ready = DBD905, 
         frozen = DBD910) %>% 
  filter(!fast %in% c(5555, 7777, 9999),
         !ready %in% c(6666, 7777, 9999),
         !frozen %in% c(6666, 7777, 9999))


diet_behavior_2014 =
  read_xpt("data/diet_2014.xpt") %>% 
  select(seqn = SEQN, 
         fast = DBD900, 
         ready = DBD905, 
         frozen = DBD910) %>% 
  filter(!fast %in% c(5555, 7777, 9999),
         !ready %in% c(6666, 7777, 9999),
         !frozen %in% c(6666, 7777, 9999))


diet_behavior = rbind(diet_behavior_2014, diet_behavior_2015)

body_measures_2015 = 
  read_xpt("data/body_measures.xpt") %>% 
  select(seqn = SEQN, 
         bmi = BMXBMI)



body_measures_2014 = 
  read_xpt("data/body_measures_2014.xpt") %>% 
  select(seqn = SEQN, 
         bmi = BMXBMI)

body_measures = rbind(body_measures_2014, body_measures_2015)


diabetes_2015 =
  read_xpt("data/diabetes.xpt") %>% 
  select(seqn = SEQN, 
         diabete = DIQ010) %>% 
  mutate(diabete = ifelse(!(diabete %in% c(1,2,3)), NA, diabete))


diabetes_2014 =
  read_xpt("data/diabetes_2014.xpt") %>% 
  select(seqn = SEQN, 
         diabete = DIQ010) %>% 
  mutate(diabete = ifelse(!(diabete %in% c(1,2,3)), NA, diabete))

diabetes = rbind(diabetes_2014, diabetes_2015)



medical_cond_2015 =
  read_xpt("data/medical_cond.xpt") %>% 
  select(seqn = SEQN, 
         overweight = MCQ080, 
         gout = MCQ160N) %>% 
  mutate(overweight = ifelse(!(overweight %in% c(1,2)), NA, overweight),
         gout = ifelse(!(gout %in% c(1,2)), NA, gout))

medical_cond_2014 =
  read_xpt("data/medical_cond_2014.xpt") %>% 
  select(seqn = SEQN, 
         overweight = MCQ080, 
         gout = MCQ160N) %>% 
  mutate(overweight = ifelse(!(overweight %in% c(1,2)), NA, overweight),
         gout = ifelse(!(gout %in% c(1,2)), NA, gout))

medical_cond = rbind(medical_cond_2014, medical_cond_2015)



coronary_2015 = 
  read_xpt("data/medical_cond.xpt") %>% 
  select(seqn = SEQN, 
         disease = MCQ160C) %>% 
  mutate(disease = ifelse(!(disease %in% c(1,2)), NA, disease))


coronary_2014 = 
  read_xpt("data/medical_cond_2014.xpt") %>% 
  select(seqn = SEQN, 
         disease = MCQ160C) %>% 
  mutate(disease = ifelse(!(disease %in% c(1,2)), NA, disease))

coronary = rbind(coronary_2014, coronary_2015)


full_data = plyr::join_all(list(demographics, income, depression, diabetes, smoking, diet_behavior, alcohol, medical_cond, coronary, body_measures), by = "seqn", type = "inner") %>% 
  filter(!is.na(disease)) %>% 
  dplyr::select(-seqn) %>% 
  na.omit



# 6115 records, 18 variables 
```

##### Recoding
now we are changing yes to 1 and no to 0. changing all binary to 0 1 and all categorical to factors

```{r}
  
full_data_binary_subset = full_data %>% 
  select(c(gender, citizen, smoked, drinker, heavy_drinker,overweight,gout, disease))

full_data_non_binary = full_data %>%
  select(-c(gender, citizen,smoked, drinker, heavy_drinker,overweight,gout, disease))

full_data = cbind(map_dfc(full_data_binary_subset, ~as.factor(ifelse(.x==2, 0, .x))), full_data_non_binary)%>%
  mutate(race = as.factor(race),
         income = as.factor(income),
         education = as.factor(education),
         diabete = as.factor(diabete)) %>% 
  mutate(disease = recode(disease, "0" = "neg", "1" = "pos"),
         disease = relevel(disease, "pos")) %>% 
  select(disease, everything())

# write.csv(full_data, "data/full_data.csv", row.names = FALSE)
```

full_data has 17 predictors.

###### data preprocessing

```{r pre}

# remove low variance predictors
x_full =  model.matrix(disease~., full_data)[,-1]

x = x_full[, -nearZeroVar(x_full)]   # 23 predictors: remove gout1 race7 diabete3
```


###### split training and test dataset


```{r}
set.seed(1)
rowTrain <- createDataPartition(y = full_data$disease,
                                p = 0.75,
                                list = FALSE)

data_train = full_data[rowTrain,]
data_test = full_data[-rowTrain,]

#data_train = data_train %>% mutate(disease = relevel(disease, "pos"))

#data_test = data_test %>% mutate(disease = relevel(disease, "pos"))


x_train = x[rowTrain,]
x_test = x[-rowTrain,]

y_train = data_train$disease
y_test = data_test$disease
```





Support vector machines is used to find a hyperplane that separate the two-class in feature space.
Here we consider two types of SVM: linear kernal and radial kernel. For linear kernal, there is one tunning parameter cost, which allows a soft margin. The tolerance for observations being on the wrong side of the margin increases and the margin becomes wild as cost increases. For radial kernel, there are two tunning parameters: sigma and cost. The fit becomes more non-linear as sigma increase. We use 10-fold cross validation to pick the best value of tunning parameter. 
One thing need to take into consideration is that SVM doesnâ€™t compute the probability, it is like just drawing the plane to split the points, so we can only use accracy and kappa as metric. Also if we want to compare the SVM with other models, we can only use kappa and accuracy as metric not ROC.

From the result of cross validation using kappa as metric, for linear kernal, the best tuning parameter is cost =0.01831564, for radical kernal, the best tuning parameters are sigma = 0.02941308 and cost = 92.41745.


```{r}
ctrl1 = trainControl(method = "cv")
```

### Support vector classifier (linear kernal)
```{r}
# support vector classifier (linear kernal) on training data
set.seed(1)
svml.fit <- train(x_train,
                  y_train,
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-4,3,len=20))),
                  metric = "Kappa",
                  trControl = ctrl1)

ggplot(svml.fit, highlight = TRUE)

saveRDS(svml.fit, "svml.fit.RDS")
```

From the result of 10-fold cross-validation, the optimal tuning parameter is:

```{r}
svml.fit$bestTune
```

Test data performance:
```{r}
pred.svml.test <- predict.train(svml.fit, newdata = x_test)
confusionMatrix(data = pred.svml.test, 
                reference = y_test)

```


### Support vector machine (radial kernel)

```{r, warning=FALSE}
# Support vector machine (radial kernel) on the training data
svmr.grid <- expand.grid(C = exp(seq(-4,5,len=20)),
                         sigma = exp(seq(-9,-1,len=20)))
set.seed(1)             
svmr.fit <- train(x_train,
                  y_train,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  metric = "Kappa",
                  trControl = ctrl1)

ggplot(svmr.fit, highlight = TRUE)

saveRDS(svmr.fit, "svmr.fit.RDS")
```

From the result of 10-fold cross-validation, the optimal tuning parameter is:

```{r}
svmr.fit$bestTune
```


Test data performance:
```{r}
pred.svmr.test <- predict.train(svmr.fit, newdata = x_test)
confusionMatrix(data = pred.svmr.test, 
                reference = y_test)

```

Model selection:

```{r}
resamp <- resamples(list(svml = svml.fit, svmr = svmr.fit))
summary(resamp)
bwplot(resamp)
```

## Undersampling

# run the code below 
```{r}
under_data = readRDS("under_data.rds")

set.seed(1)
under_rowTrain <- createDataPartition(y = under_data$disease,
                                p = 0.75,
                                list = FALSE)
under_x_full =  model.matrix(disease~., under_data)[,-1]

under_x = under_x_full[, -nearZeroVar(under_x_full)]   # 24 predictors: remove race7 diabete3


under_data_train = under_data[under_rowTrain,]
under_data_test = under_data[-under_rowTrain,]


under_x_train = under_x[under_rowTrain,]
under_x_test = under_x[-under_rowTrain,]

under_y_train = under_data_train$disease
under_y_test = under_data_test$disease

under_xy_train = cbind(as.data.frame(under_x_train), under_data_train$disease) %>%
  rename(disease = `under_data_train$disease`)
under_xy_test = cbind(as.data.frame(under_x_test), under_data_test$disease) %>%
  rename(disease = `under_data_test$disease`)

````

```{r}
ctrl1 = trainControl(method = "cv")
```


```{r}
# support vector classifier (linear kernal) on undersampling training data
set.seed(1)
svml.fit.under <- train(under_x_train,
                  under_y_train,
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-4,3,len=20))),
                  metric = "Kappa",
                  trControl = ctrl1)

ggplot(svml.fit.under, highlight = TRUE)
saveRDS(svml.fit.under, "svml.fit.under.RDS")
```

From the result of 10-fold cross-validation, the optimal tuning parameter is:

```{r}
svml.fit.under$bestTune
```

Test data performance:
```{r}
pred.svml.test.under <- predict.train(svml.fit.under, newdata = under_x_test)
confusionMatrix(data = pred.svml.test.under, 
                reference = under_y_test)

```



```{r, warning=FALSE}
# Support vector machine (radial kernel) on the undersampling training data
svmr.grid <- expand.grid(C = exp(seq(-4,5,len=20)),
                         sigma = exp(seq(-9,-1,len=20)))
set.seed(1)             
svmr.fit.under <- train(under_x_train,
                  under_y_train,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  metric = "Kappa",
                  trControl = ctrl1)

ggplot(svmr.fit.under, highlight = TRUE)

saveRDS(svmr.fit.under, "svmr.fit.under.RDS")

```

From the result of 10-fold cross-validation, the optimal tuning parameter is:

```{r}
svmr.fit.under$bestTune
```


Test data performance:
```{r}
pred.svmr.test.under <- predict.train(svmr.fit.under, newdata = under_x_test)
confusionMatrix(data = pred.svmr.test.under, 
                reference = under_y_test)

```

```{r}
resamp <- resamples(list(svml = svml.fit.under, svmr = svmr.fit.under))
summary(resamp)
bwplot(resamp)
```



## Unsupervise

For unsupervise learning, we only focus on the 17 predictors: 12 continuous features and 5 categorical features. We try to discover the unknown subgroups in data using clustering and do data visualization which finds a low-dimensional representation of the data that contains as much as possible of the variation. 

```{r}
# 17 features
dat = 
  full_data %>% 
  select(-disease)

# 5 binary features
dat_c = 
  dat %>% 
  select(age, depression_score, fast, ready, frozen, bmi)
dat_c = scale(dat_c)

# design matrix
x
```

### Clustering

```{r}
fviz_nbclust(x, FUNcluster = cluster::pam,method = "silhouette")
set.seed(1)
pam = pam(x, k=2)
pam_vis = fviz_cluster(list(data = x, cluster = pam$clustering), 
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "PAM")

pam_vis
```

For clustering, k-means and hierarchical clustering were taken into consideration. However, k-means clustering should not be done with data of mixed types. Since k-means is essentially find partitions that minimizes the within-cluster squared Euclidean distances between the clustered observations and the cluster centroid, it should only be used with data where squared Euclidean distances would be meaningful. 

One popular choice is PAM (partitioning around medoids), which is essentially the same as k-means, but is intended to find a sequence of objects called medoids that are centrally located in clusters rather than the centroid. We used fviz_nbclust() to determines and visualized the optimal number of clustering, the result showed that 2 cluster is with the highest average silhouette width, this is consistent with our purpose, since we want to group our data with one cluster for high risk of heart disease and another for low risk. However, from the result of ggplot2_based visualization of PAM, based on the first and second PC for each points, two clusters are not well seperated. Which may indiate that there isn't much clustering for our data.


Hierarchical clustering is another way for clustering that no need to pre-specify the number of clusters and based on the dissimilarity of each pair of data. Since our data is mixed with continuous variables and categorical variables, we cannot use Euclidean distance, Hanhattan distance or Minkowski distance to measure the dissimilarity. The challenge with categorical variables is to find a suitable way to represent distances between variable categories and individuals in the factorial space.  One way is to use Gower's distance, which is a composite measure.
[Gower J. C. A general coefficient of similarity and some of its properties // Biometrics, 1971, 27, 857-872]

However, since we have 6067 observations, it is really hard to plot the cluster dendrogram for our dataset, hence it is hard to evaluate and interpret the results from hierarchical.
```{r}
library(cluster)
```

```{r}
g.dist = daisy(dat, metric="gower", stand = FALSE, type = list())

summary(g.dist)

hc.complete <- hclust(g.dist, method = "complete")
hc.average <- hclust(g.dist, method = "average")
hc.single <- hclust(g.dist, method = "single")
hc.centroid <- hclust(g.dist, method = "centroid")
```

```{r}
fviz_dend(hc.complete, k = 2,        
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

ind4.complete <- cutree(hc.complete, 2)


# Who are in the fourth cluster?
plot(dat[ind4.complete == 2,])
```



### PCA


Principal Components Analysis can also served as a tool for data visualization, which find orthogonal directions of the highest variance, and the first two PCs of the data span the plane that is cloest to the n observations.

Since our features contain both continuous variables and categorical variables, PCA cannot directly applied to deal with this mixed type of data. The PCA is really analysis of eigenvectors of covariance matrix. So the problem is how to calculate the "correct" covariance matrix for mixed data. One of the approaches is to use polychoric correlation for categorical variables. Another approach is look for a non-linear transformation of each variable--whether it be nominal, ordinal, polynomial, or numerical--with optimal scaling. 

Here we only consider 5 continuous feactures, so we can directly use PCA. We focus on the first two PCs for contribution plot, fast (number of fast food meals eaten weekly) and age contribute most the first PC, bmi and depression score contribute most for second PC.

Figure[] and Figure[] showed the graph of individuals and variables.


```{r}
pca = prcomp(dat_c)
pca$rotation

var = get_pca_var(pca)
corrplot(var$cor)

fviz_eig(pca, addlabels = TRUE)

library(gridExtra)
a <- fviz_contrib(pca, choice = "var", axes = 1)
b <- fviz_contrib(pca, choice = "var", axes = 2)
grid.arrange(a, b, nrow = 2)

fviz_pca_var(pca, col.var = "steelblue", repel = TRUE)
fviz_pca_ind(pca,
             habillage = ifelse(full_data_binary_subset$disease==1,"Heart Disease","Non Heart Disease"),
             label = "none",
             addEllipses = TRUE)

```







```{r}
## rf with roc
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

rf.grid <- expand.grid(mtry = 1:14,
                       splitrule = "gini",
                       min.node.size = 1:10)
set.seed(1)
rf.fit <- train(x_train,
                y_train, 
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

saveRDS(rf.fit, "rf.fit_roc.RDS")

plot(rf.fit)

rf.fit$bestTune


rf.pred <- predict.train(rf.fit, newdata = x_test, type = "prob")[,1]
library(pROC)
roc.rf <- roc(y_test, rf.pred, levels = c("neg", "pos"))
print(roc.rf)

```



